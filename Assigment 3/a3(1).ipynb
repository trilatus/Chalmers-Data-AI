{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcnrUORIM54F"
      },
      "source": [
        "# Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting by extracting the emails by using the librarie tarfile."
      ],
      "metadata": {
        "id": "HjpyzB0WpaQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "for file in os.listdir(\"/content/\"):\n",
        "  if file.endswith(\".bz2\"):\n",
        "    with tarfile.open(file, \"r:bz2\") as tar:\n",
        "      tar.extractall(path = \"tmp\")\n",
        "\n",
        "extracted_folders = os.listdir(\"/content/tmp\")\n",
        "print(\"Extracted folders:\", extracted_folders)"
      ],
      "metadata": {
        "id": "DhKROP6vM1nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed18a50-2782-447d-858b-d457ae477aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted folders: ['easy_ham', 'spam', 'hard_ham']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBIbaYCOmKw"
      },
      "source": [
        "We extracted the files from their bz2 format and now we are going to export the emails to induvidual text files to be able to analyze thier content and so one can supply an anwser to question A."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import email\n",
        "from email import policy\n",
        "from email.parser import BytesParser\n",
        "from email.header import decode_header\n",
        "\n",
        "for folder in extracted_folders:\n",
        "  path = \"/content/tmp/\" + folder\n",
        "  file_list = os.listdir(path)\n",
        "\n",
        "#Opening one folder at the time and then reading and writing each file to a .txt based on the folder name\n",
        "#In the end genereates three .txt that can be read and analyzed for question A\n",
        "  for file_name in file_list:\n",
        "    with open(path + \"/\" + file_name, \"rb\") as file:\n",
        "      with open(folder + \".txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "        msg = BytesParser(policy=policy.default).parse(file)\n",
        "\n",
        "        if msg[\"subject\"] is None:\n",
        "          continue\n",
        "        else:\n",
        "          f.write(f\"Subject: {msg['subject']}\\n\")\n",
        "        f.write(f\"From: {msg['from']}\\n\")\n",
        "\n",
        "        if msg['to'] is None:\n",
        "          continue\n",
        "        else:\n",
        "          f.write(f\"To: {msg['to']}\\n\")\n",
        "        body = msg.get_body(preferencelist=('plain',))\n",
        "\n",
        "        if body is not None:\n",
        "          try:\n",
        "              content = body.get_content()\n",
        "              charset = body.get_content_charset() or \"utf-8\"\n",
        "              #Attempt to decode using the detected charset if not fallback will apply\n",
        "              content = content.encode(\"ascii\", \"ignore\").decode(charset, errors=\"replace\")\n",
        "              f.write(f\"Body:\\n{content}\\n\")\n",
        "          except LookupError:\n",
        "\n",
        "              f.write(\"Body: [Could not decode the body due to unknown encoding]\\n\")\n",
        "\n",
        "          #Email seperator for easy reading\n",
        "          f.write(\"###########################################################\\n\")"
      ],
      "metadata": {
        "id": "A7elnPcaMs_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGNs3HccbpP6"
      },
      "source": [
        "# Problem 1: Question A\n",
        "Starting by looking at the readme.html provided by spamassassin its stated that the easy_ham emails dont frequently contain spammish signatures which can be lots of capital letters, html and keyword like \"adult-only\" etc. Hard_ham on the other hand have serveral aspects close to spam. So some of the signatures of spam can be found in the emails, which makes them harder to detect as non spam. And the spam file includes spam.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc4U3yazaNSH"
      },
      "source": [
        "**Sample mail from the txt files can be seen below:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Subject: Rats' intestines and pigs' teeth\n",
        "From: boingboing <rssfeeds@example.com>\n",
        "To: yyyy@example.com\n",
        "Body:\n",
        "URL: http://boingboing.net/#85497383\n",
        "Date: Not supplied\n",
        "\n",
        "This is the headline of the month, possibly the year: \"Doctors Grow Pig Teeth\n",
        "in Rat Intestines.\" Do we even need to read the story to understand it? It's\n",
        "like a freaking _haiku_ of near-singularity, future-shocky wonderment!\n",
        "\n",
        "    U.S. doctors said on Thursday they have managed to grow living pig teeth in\n",
        "    rats, a feat of biotechnology that experts said could spark a dental\n",
        "    revolution.\n",
        "\n",
        "    Researchers at Boston's Forsyth Institute said their successful experiment\n",
        "    suggests the existence of dental stem cells, which could one day allow a\n",
        "    person to replace a lost tooth with an identical one grown from his or her\n",
        "    own cells.\n",
        "\n",
        "    \"The ability to identify, isolate and propagate dental stem cells to use in\n",
        "    biological replacement tooth therapy has the potential to revolutionize\n",
        "    dentistry,\" said Dominick DePaola, president and CEO of the institute that\n",
        "    focuses on oral and facial science.  \n",
        "\n",
        "Link[1] Discuss[2] (_Thanks, Dave[3]!_)\n",
        "\n",
        "[1] http://story.news.yahoo.com/news?tmpl=story&ncid=585&e=1&cid=585&u=/nm/20020926/sc_nm/health_teeth_dc\n",
        "[2] http://www.quicktopic.com/boing/H/88cUs6CumjiX\n",
        "[3] http://www.remtullaeurorscg.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoTVzUKNvHHA"
      },
      "source": [
        "# Problem 1: Question B\n",
        "Start with creating python lists that include spam and easy-ham and the other will also include spam and hard-ham. The following code have resamblens to the code seen in \"Problem 1\" but for more structured code this was the aproch used. After the mails are put into the list the data spliting is perfomed. As mentioned in the lecture the trainset consist of 75% of the mails and only 25% for the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU_Dhz_m8Rkb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection as ms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Create 2 list which is gonna hold mail from easy and hard, both the list will also contain spam for the data splitting.\n",
        "lst_ez_ham = []\n",
        "lst_hd_ham = []\n",
        "lst_spam = []\n",
        "\n",
        "for folder in extracted_folders:\n",
        "  for file in os.listdir(\"/content/tmp/\" + folder + \"/\"):\n",
        "    with open(\"/content/tmp/\" + folder + \"/\" + file, \"rb\") as f:\n",
        "      data = f.readlines()\n",
        "      data = [line.decode('utf-8', errors='ignore') for line in data]  # Decode binary data to text\n",
        "      content = ''.join(data)  # Join lines into a single string\n",
        "\n",
        "\n",
        "      if folder == \"easy_ham\":\n",
        "        lst_ez_ham.append(content)\n",
        "      elif folder == \"hard_ham\":\n",
        "        lst_hd_ham.append(content)\n",
        "      else:\n",
        "        lst_spam.append(content)\n",
        "\n",
        "\n",
        "#Rule of thumb: if we do a simple train-test split, use 75% of data for the training set and 25% for the test set\n",
        "\n",
        "#Easy Ham Split datasplit\n",
        "X_ez = lst_ez_ham + lst_spam\n",
        "y_ez = [0] * len(lst_ez_ham) + [1] * len(lst_spam) #Labels\n",
        "\n",
        "X_ez_train, X_ez_test, y_ez_train, y_ez_test = train_test_split(X_ez, y_ez, test_size=0.25, random_state=64, stratify=y_ez)\n",
        "\n",
        "#Hard Ham Split datasplit\n",
        "X_hd = lst_hd_ham + lst_spam\n",
        "y_hd = [0] * len(lst_hd_ham) + [1] * len(lst_spam) #Labels\n",
        "\n",
        "X_hd_train, X_hd_test, y_hd_train, y_hd_test = train_test_split(X_hd, y_hd, test_size=0.25, random_state=64, stratify=y_hd)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labels are important for the model to differentiate between ham and spam based on the content in the training dataset."
      ],
      "metadata": {
        "id": "J_03WaUQwHBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Preprocessing"
      ],
      "metadata": {
        "id": "rx8EwD_vn_1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMnreTtW0FmV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s6CVWwTLVtc"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"dataset/spam\", exist_ok=True)\n",
        "os.makedirs(\"dataset/ham\", exist_ok=True)\n",
        "with tarfile.open(\"/content/dataset/easy_ham.tar.gz\") as tar_ref:\n",
        "    tar_ref.extractall(\"/content/dataset/easy_ham\")\n",
        "    tar_ref.close()\n",
        "\n",
        "with tarfile.open(\"/content/dataset/spam.tar.gz\") as tar_ref:\n",
        "    tar_ref.extractall(\"/content/dataset/spam\")\n",
        "    tar_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we do some preprocessing. We create labels - 1 for ham and 0 for spam. We also create and training and test split with the the help of train_test_split(). We set the random state value to 64 so that when we run the program again we get the same split so that we can reproduce results."
      ],
      "metadata": {
        "id": "i6ge3bv27kTe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqwLs9BR44D5"
      },
      "outputs": [],
      "source": [
        "spam_list = []\n",
        "ham_list = []\n",
        "\n",
        "for file in os.listdir(\"/content/dataset/spam/spam\"):\n",
        "    with open(os.path.join(\"/content/dataset/spam/spam\", file), 'r', errors=\"ignore\") as file:\n",
        "        spam_list.append(file.read())\n",
        "\n",
        "for file in os.listdir(\"/content/dataset/easy_ham/easy_ham\"):\n",
        "   with open(os.path.join(\"/content/dataset/easy_ham/easy_ham\", file), 'r', errors=\"ignore\") as file:\n",
        "       ham_list.append(file.read())\n",
        "\n",
        "ham_labels = [1] * len(ham_list)\n",
        "spam_labels = [0] * len(spam_list)\n",
        "\n",
        "all_mail = spam_list + ham_list\n",
        "all_labels = ham_labels + spam_labels\n",
        "\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "    all_mail, all_labels, test_size=0.25, random_state=64, stratify=all_labels\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Easy Ham\n",
        "Here we perform the actual training of the model and then predict. We first use fit_transform(train_files) to train the model on the training data. We then transform the model, using transform(test_files) (not fit_transform as that would fit the model to the test data thus making the test useless) and finally make a prediction and observe the metrics.\n",
        "\n",
        "We also have a .75/.25 training/test split as suggested in the lecture."
      ],
      "metadata": {
        "id": "Dx6cviy98p0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "x_train = vectorizer.fit_transform(train_files)\n",
        "x_test = vectorizer.transform(test_files)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(x_train.toarray())\n",
        "\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_train, train_labels)\n",
        "test_predictions = clf.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f\"Accuracy for Multinomial: {accuracy * 100:.2f}%\")\n",
        "############################################################################################\n",
        "\n",
        "clfB = BernoulliNB()\n",
        "clfB.fit(x_train, train_labels)\n",
        "testB_predictions = clfB.predict(x_test)\n",
        "accuracyB = accuracy_score(test_labels, testB_predictions)\n",
        "print(f\"Accuracy for Bernoulli: {accuracyB * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "QGvqZAuk8jm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "bc03d138-9c3c-401a-a225-4016bcf3c843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c45c291eda7f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the accuracy for both models. We also got different accuracies for different values for random_states, which makes sense (to some degree)."
      ],
      "metadata": {
        "id": "_e-IAhYOFaiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4: Hard Ham"
      ],
      "metadata": {
        "id": "SAxGId1ZpMQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "x_hd_train = vectorizer.fit_transform(X_hd_train)\n",
        "x_hd_test = vectorizer.transform(X_hd_test)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(x_hd_train.toarray())\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_hd_train, y_hd_train)\n",
        "testA_predictions = clf.predict(x_hd_test)\n",
        "accuracy = accuracy_score(y_hd_test, testA_predictions)\n",
        "print(f\"Accuracy for Multinomial: {accuracy * 100:.2f}%\")\n",
        "\n",
        "clfB = BernoulliNB()\n",
        "clfB.fit(x_hd_train, y_hd_train)\n",
        "testB_predictions = clfB.predict(x_hd_test)\n",
        "accuracyB = accuracy_score(y_hd_test, testB_predictions)\n",
        "print(f\"Accuracy for Bernoulli: {accuracyB * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q80dbEwhuC92",
        "outputId": "e02f4771-4084-41ff-d284-9122a992b14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' ... '쵥λ' '𬣡b' '𬣤jӡa']\n",
            "[[ 3  0  2 ...  0  0  0]\n",
            " [ 1  0  0 ...  0  0  0]\n",
            " [ 1  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 3  0  0 ...  0  0  0]\n",
            " [12  0  8 ...  0  0  0]]\n",
            "Accuracy for Multinomial: 92.55%\n",
            "Accuracy for Bernoulli: 84.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Easy Ham"
      ],
      "metadata": {
        "id": "JIgnopABkANJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "x_ez_train = vectorizer.fit_transform(X_ez_train)\n",
        "x_ez_test = vectorizer.transform(X_ez_test)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(x_ez_train.toarray())\n",
        "print(\"\")\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_ez_train, y_ez_train)\n",
        "testA_predictions = clf.predict(x_hd_test)\n",
        "\n",
        "accuracy = accuracy_score(y_ez_test, testA_predictions)\n",
        "precision = precision_score(y_ez_test, testA_predictions)\n",
        "recall = recall_score(y_ez_test, testA_predictions)\n",
        "print(\"The Multinomial Naive Bayesian Classifier\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_ez_test, testA_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\")\n",
        "#############################################################\n",
        "\n",
        "clfB = BernoulliNB()\n",
        "clfB.fit(x_ez_train, y_ez_train)\n",
        "testB_predictions = clfB.predict(x_hd_test)\n",
        "\n",
        "accuracy = accuracy_score(y_ez_test, testB_predictions)\n",
        "precision = precision_score(y_ez_test, testB_predictions)\n",
        "recall = recall_score(y_ez_test, testB_predictions)\n",
        "print(\"The Bernoulli Naive Bayesian Classifier\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_ez_test, testB_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVUH46XMgcR7",
        "outputId": "b76cf6bb-74a9-4389-8a90-eda5ffcb83a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' ... '쵥λ' '𬣡b' '𬣤jӡa']\n",
            "[[0 0 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [3 0 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [3 0 1 ... 0 0 0]]\n",
            "\n",
            "The Multinomial Naive Bayesian Classifier\n",
            "Accuracy: 96.85%\n",
            "Precision: 98.10%\n",
            "Recall: 82.40%\n",
            "Confusion Matrix:\n",
            "[[636   2]\n",
            " [ 22 103]]\n",
            "\n",
            "The Bernoulli Naive Bayesian Classifier\n",
            "Accuracy: 90.69%\n",
            "Precision: 100.00%\n",
            "Recall: 43.20%\n",
            "Confusion Matrix:\n",
            "[[638   0]\n",
            " [ 71  54]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rx8EwD_vn_1Q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}